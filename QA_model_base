# -*- coding: utf-8 -*-
"""
Created on Wed Dec  6 17:32:50 2023

@author: Administrator
"""
# 用anaconda的base本地环境设置一个通道实例：xinference -p 9997
# 部署模型
from xinference.client import Client
client = Client("http://localhost:9997")
model_uid = client.launch_model(model_name="bge-base-zh-v1.5", model_type="embedding")
model = client.get_model(model_uid)
# print(model.create_embedding("write a poem."))


# 用Xinference Embeddings将知识库向量化
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import DataFrameLoader
import pandas as pd
# loader = TextLoader("https://github.com/seal1995/NLP/blob/main/QA.txt") # 替换成任何你想要进行问答的https://github/.txt文件
df = pd.read_excel(r"C:\Users\Administrator\PycharmProjects\Pycharmuse\大模型\问答数据.xlsx",sheet_name='data',usecols=["data"])
loader = DataFrameLoader(df, page_content_column="data")
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1000,
    chunk_overlap  = 0,
    length_function = len,
)
docs = text_splitter.split_documents(documents)

from langchain.embeddings import XinferenceEmbeddings

xinference_embeddings = XinferenceEmbeddings(
    server_url="http://127.0.0.1:9997", # 换成设置的url，这里用的是默认端口
    model_uid = {model_uid}  # 替换成之前返回的模型model_uid
)


# 用Milvus实现向量搜索
from langchain.vectorstores import Milvus

vector_db = Milvus.from_documents(
    docs,
    xinference_embeddings,
    connection_args={"host": "127.0.0.1", "port": "19530"},
)
# query = ""
query = "what does the president say about Ketanji Brown Jackson"
docs = vector_db.similarity_search(query, k=10)
print(docs[0].page_content) 
